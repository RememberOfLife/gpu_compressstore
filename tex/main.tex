%\documentclass{tudscrartcl}
\documentclass{tudscrreprt}
%\documentclass[ngerman, 11pt, a4paper, twoside, cleardoublepage=empty, open=right, numbers=noenddot, cd=lightcolor, final]{tudscrreprt}

% fix bib
\usepackage[style=numeric,sorting=nyt]{biblatex}
\addbibresource{main.bib}

\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}

% for cpp
\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{microtype}

% review highlighted text
\usepackage{xcolor}
\definecolor{review}{RGB}{221,33,114}
\newcommand{\markr}[1]{\textcolor{review}{$\langle$#1$\rangle$}}

\begin{document}
	
	\faculty{Fakultät Informatik}
	\department{Institut für Systemarchitektur}
	\chair{Lehrstuhl für Datenbanken}
	\date{15.06.2021}
	\title{Tailoring Compressstore to GPU}
	\thesis{bachelor} \graduation[BSc.-Inf.]{Bachelor-Informatik}
	\author{ Urs Kober
		\matriculationnumber{---}
		\dateofbirth{DD.MM.YYYY}
		\placeofbirth{---}
	}
	\matriculationyear{2018}
	\supervisor{Prof. Dr.-Ing. habil. Dirk Habich \and Dipl.-Inf. Johannes Fett}
	\professor{Prof. Dr.-Ing. habil. Wolfgang Lehner}
	\maketitle
	\confirmation
	
	\chapter*{Abstract}
		\begin{itemize}
			\item work deals with compressstore on gpu
			\item analyze performance considerations
			\item compare to existing cub
			\item match cub in general case by ~2ms
			\item improvement over cub in sparse (edge/special) cases
		\end{itemize}
	
	\tableofcontents
	
	\chapter{Introduction}
		With the rise of increasingly capable computing hardware and often even faster growing data collection mechanisms, modern data refinement is not an easy task anymore. Recent years have also shown a trend of using general purpose graphics processing units (GPUs or GPGPUs) to accelerate traditional easily parallelizable workloads. \\
		
		The compressstore operation that this work deals with fitting to the GPU, also has applications in the mentioned data refinement workflows. \\
		Most obvious is the use as a database projection operation, selecting only some columns of data from a larger set. This non general operational mode of a compressstore can be optimized for every given pattern at runtime. \\
		A more general use might be to reduce the size of a problem between single steps of a process. This may be after certain filters have been applied to an intermediate dataset and excess data is best discarded to reduce unnecessary workload on successive processing steps. When the data to remove is not partioned in a regular fashion the issue calls for the compressstore operation. \\
		Similar use, although on a different granularity, is required at enormous data collection facilities such as a CERN detector \cite{cern_datastreaming}. \\
		
		Viability of this relies heavily on fast runtimes for a procedure designed to save even more time overall. Catering to this need is the massively parallel computing capability of modern GPUs, which can be utilized to a great extent with the compressstore operation. This parallelization requires careful tuning of GPU specific parameters to realize the highest performance gains. \\
		
		Regarding database uses, the input masks used for the operation may not always be uniformly distributed. Some of these non uniform masktypes may contain even more potential for optimization. \\
		
	\chapter{Foundations}
		In this section we will introduce key concepts required to understand the work and its functioning. Specifically we start by defining the compressstore problem and its nuances, before providing a broad overview of the Nvidia CUDA GPU architecture and execution model as it was used to develop and compare the implementations. For usage as baseline comparisons we also briefly look at AVX and the existing CUB implementation.
	
		\section{Compressstore}
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/compressstore_operation.png}
				\caption{\label{fig:compressstore_operation}Visualized compressstore operation on a 16 element long data vector and bitmask. Any values of "Don't Care" elements in the output is not relevant to the operation.}
			\end{figure}
			The compressstore operation is used to store certain elements of an input vector contiguously in memory. Using the provided bitmask the operation only selects elements for writeout that have a corresponding enabled bit (set to one) in the bitmask. Potential gaps between enabled elements are removed, thereby left packing the results into the output. The length of the output vector will always be less than or equal to that of the input vector. \\
			Even though not all applications of this operation require the same assumptions, in this work a few common ones are made.
			\begin{itemize}
				\item The input vector of elements may not be overwritten.
				\item The order of elements is kept stable through the writeout process.
				\item There is no information about the bitmask in advance.
			\end{itemize}
			Parallelization of this operation is not-entirely trivial due to requiring for every input element the writeout location in the output vector as a count of the previous enabled inputs, before processing can occur. \\
			Depending on the density and distribution of bits set to one in the bitmask there are various special cases, some of which may benefit from specific acceleration. \\
			These include but are not limited to:
			\begin{itemize}
				\item very sparse or dense bitmasks (with uniform distribution)
				\item zipf distributed bitmasks as they might spawn from an ordered group by query filter
				\item burst masks as they might spawn from a tablescan over data with temporal dependency
				\item or repeating pattern bitmasks, useful for performing a project operation.
			\end{itemize}
			The overall density of a bitmask in terms of a ration $\frac{\text{one bit count}}{\text{total element count}}$ will be called $p$ in the rest of this work. It should be noted that this does not give any indication as to what distribution is present in the bitmask. A bitmask where the entirety of the second half of the bitmask is set to one bits and no others, will still yield $p=0.5$ just like a uniform random distribution might. \\
		
		\section{GPU Computing}
			\subsection{Architecture}
				\begin{figure}[!ht]
					\centering
					\includegraphics[width=0.95\linewidth]{./img/gpu_architecture.png}
					\caption{\label{fig:gpu_architecture}Simplified GPU architecture diagram. In reality the relative position and size of many components frequently changes with each generation. Some parts are omitted for clarity. The actual number of cuda cores per streaming multiprocessor (SM) is higher in real devices.}
				\end{figure}
				GPUs have, as their name implies, been originally devised as a dedicated device for graphical computation workloads, especially in realtime scenarios \cite{cuda_arch}. They have long since their inception also seen much use as accelerators for general purpose computing workloads, many of which benefit greatly from the high parallelism they provide. \\
				Modern GPU architecture consists, across different hardware manufacturers, of mostly differently named but functionally very similar components, the basics of which are described here, to give a brief overview of the structures available to improve performance. \\
				
				Similar to the RAM of a processor, a typical Nvidia GPU has a global memory shared among all of its processing power, even device intrinsic management functions. Between interactions of processing elements and the global memory is an L2 cache and multiple memory controllers. \\
				Communication with the host is performed through the PCIe host interface. \\
				
				The compute unified device architecture (CUDA) of all recent Nvidia GPUs \cite{cuda_smcc} allows generalisations about the design of the GPUs processing power, helping developers easier target multiple devices with fine tuned applications. \\
				
				When executing code on the GPU, several parallel and concurrent threads all execute the same code. Discernible difference for accessing inputs and outputs is provided solely through a running count across all threads of a workload. More on this and the use of branches in \ref{sec:gpu_execution_model}. \\
				When launching a kernel (the workload) that is meant to run on the GPU, the user specifies a number of blocks (grid dimension) and a number of threads per block (block dimension) for the GPU to launch. \\
				Scheduling of blocks on the GPU is managed by the GigaThread engine, specially designed to handle the immensely parallel tasks assigned to GPUs. It distributes the outstanding blocks of running kernels onto the many streaming multiprocessors (SMs) of the GPU, which is the key structure of processing, of which several dozens can be found in modern devices. When a block of threads is assigned to an SM, the threads it contains are grouped into warps of 32 each. \\
				
				An SM contains many so called CUDA cores, which are the real execution units used by warps running on the SM. Every thread alive on an SM has its own registers in a very large register file that serves the entire SM. To distribute the work that threads want to perform across the CUDA cores the SM uses several warp schedulers, that assign a whole warp of threads to execution units respective of the instruction that is being executed. \\
				Because every thread has effectively its own register file, there is a zero overhead scheduling of warps in every SM. As long as there are warps available to schedule, i.e. warps that have instructions for which there are currently available execution units in the SM, the warp schedulers issue the warps to their execution units between cycles, such that in every cycle actual work gets done. \\
				
				The CUDA cores available in every SM vary across different GPUs and GPU architecture generations, but in general there are many integer and floating point arithmetic units available, with a small number of specialized units available for computing tensor or raytracing operations as well as several hardware accelerated math functions. \\
				
				Memory accesses are grouped together into larger width transactions where possible, and performed and processed by the specialized load/store units. There exists a small L1 cache for global memory operations, which shares a configurable amount of space with the per SM shared memory. Threads on the SM can read from and write to shared memory effectively in only one cycle, however there are serialization limitations if multiple threads perform writes on the same location. \\
				Memory performance intensive applications that issue a lot of read/write instructions to the load store units (LSU) may quickly suffer of warps stalling due to a reached pipeline limit for the amount of operations in transit per LSU, especially if the widths of memory accesses are smaller, which leads to less coalesced memory accesses and therefore more individual transactions \cite{cuda_memorycoalescing}. \\
			
			\subsection{Execution Model}
				\label{sec:gpu_execution_model}
				As touched on in the architecture section, all threads of a kernel execute the same code. In the case of CUDA this is written using a C++ extension and the nvcc compiler provided by Nvidia. Such a function, that is executed by all threads, is called a kernel. To create differentiation between threads, intrinsic values for the block number/dimension and thread number inside a block can be used together to create a global thread identification. \\
				The grid and block dimensioning is not limited to one dimension, they can seperately be each up to three dimensional. \\
				
				Instructions are usually, excluding certain cases on newer devices, executed in lockstep for the entire warp at once. The single instruction multiple threads (SIMT) model of execution functions as a natural, more parallel, progression from single instruction multiple data (SIMD) as it is used on many CPUs. \\
				The SIMT model allows actual branching of code, contrary to the SIMD approach of only masking out operations of disabled elements during a vector operation. Branches in the code are serialized through branch predication, and thus very time intensive. All threads not participating in a given branch effectively perform NOPs while the other threads execute. In excess this behaviour is called warp (execution) divergence and can cause large performance losses. \\
				
				Kernel launches and other CUDA library functions, such as copying memory between the host and device may also accept a stream argument. By default these operations are serialized in the default stream. Using non-default streams they can be parallelized on the GPU, and furthermore partially synchronized using events that are triggered and waited for by streams. \\
				Using streams may enable partial processing of data while copying the rest from the host side to the device still. Or when using events, enables dependencies between larger chains of kernel launches. \\
				
				Minimal exemplary code for an increment kernel is provided below. For further information refer to the comprehensive CUDA programming guide \cite{cuda_programmingguide}.
				\begin{lstlisting}[caption=Device Side: Increment Kernel]
__global__ void increment_kernel(int* data, int N) {
	int tid = threadIdx.x + blockIdx.x * blockDim.x;
	if (tid < N) {
		data[tid] += 1;
	}
}
				\end{lstlisting}
				
				\begin{lstlisting}[caption=Host Side: Device Malloc and Kernel Launch]
int* h_data = (int*)malloc(1000*sizeof(int));
// ... h_data gets populated with numbers
int* d_data; // pointer to data on the device
cudaMalloc(&d_data, 1000*sizeof(int));
cudaMemcpy(d_data, h_data, 1000*sizeof(int), cudaMemcpyHostToDevice);
increment_kernel<<<4, 256>>>(d_data, 1000);
				\end{lstlisting}
				
			
		\section{AVX}
			Advanced Vector Extensions (AVX) \cite{avx} are SIMD extensions to the x86 instruction set architecture supported by many modern CPUs. AVX instructions are essentially hardware acceleration features for operations that otherwise require multiple non-AVX instructions. The idea of this vector processing is to execute a given operation on all elements of a fixed size vector at once. Larger problems can usually be broken down into smaller vectors that can be processed by AVX and similar vector processing instructions. \\
			Data for these operations has to be provided in special AVX registers, and is then transformed according to the used instruction in a single operation. This may take multiple clock cycles, but is usually faster than performing the operation sequentially for the inputs using normal instructions. Additionally, due to the hardware specialization, using AVX instructions may yield a better power usage profile. \\
			
			The AVX-512 extensions extend the existing AVX2 extensions from 256-bit to 512-bit vector lengths and introduce many new characteristics and instructions. Using the new AVX-512 vpcompress instructions performs a compressstore operation on the input vector, assuming input element sizes of different lengths, using a bitmask provided in another AVX register. \\
			
			Although AVX2 does not have a dedicated compressstore instruction, the behaviour of a compressstore operation over the short vector register may be reproduced using multiple AVX2 instructions. \\
		
		\section{CUB}
			CUB \cite{cub} is a library of reusable components for programming CUDA devices. It provides features for sorting, reducing, performing prefix scans, histogram creation and also a compressstore. \\
			The algorithms used in the CUB components have been fine tuned for usage with many different CUDA GPUs, and are specialized for operation not only device wide, but also spanning just a warp or block. \\
			
			The kernel used by CUB to perform the compressstore operation is designed to only schedule one wave of blocks to the device. That is, there are enough blocks to fill the device, but no additional blocks waiting to be scheduled onto an SM. This ensures that all synchronization can be done at runtime on the device, instead of relying on multiple kernel launches to synchronize dependency chains. \\
			
			It is imperative to note that the CUB flagged functionality not only performs a compressstore, but also places all masked out elements in reverse order to the back of the output vector. \\
		
	\chapter{Analysis and Concepts}
		The general compressstore problem can be split into three smaller sequential partial tasks. The first two of which only perform action on the bitmask and intermediate constructs, to enable simple parallelization of the actual writeout task, which touches the most data. \\
		
		The seemingly inherently sequential property of the compressstore, being the dependency of writeout location for an element on the count of enabled elements before it, can be parallelized using a reduction. \\
		Specifically, every enabled element is written out at an offset corresponding to the exclusive prefix sum of the bitmask at its position. \\
		
		However, because of the large amount of elements that might be processed, it is not feasible to perform any algorithm on single elements. Instead, input elements and bitmask bits are chunked together into chunks of a variable length. This drastically helps reduce the problem size for all three steps. \\
		
		To calculate the prefix sum for chunks, a population count of one bits for all preceeding chunks is required. As such the first step is to perform a popcount kernel over all chunks. Since this only requires reading from the bitmask it does not touch a lot of data, even for many elements. \\
		
		A prefix sum scan operation may be parallelized using a reduction and an expansion. The expansion step is not always necessary, as the (exclusive) prefix sum for a chunk can also be calculated using just the intermediate data from the reduction and another algorithm to perform the collection of the correct values. \\
		
		When the prefix sum is completely or partially done, the third task is to perform the writeout of elements per chunk using the pre calculated prefix sums for the chunks starting locations. This leaves the writeout task easily parallelizable, which is important because it is also the task that touches the most data. \\
		
		The emphasis on reducing the amount of touched data is important, because for the compressstore operation there are much more memory accesses than actual calculations performed. If the writeout work can be simplified by taking more time to analyze the much smaller bitmask, considerable time may be saved, especially for very sparse masks of various distributions.
	
		When working on bitmasks of repeating patterns, as they are created by a project operation, none of the prepperation steps are neccessary. The writeout location of each input element is directly available via calculation. To reduce the amount of dormant threads in a naive monolithic or grid-striding \cite{cuda_gridstriding} implementation of this for low density bitmasks, a skip table can be precalculated for the pattern, such that the writeout memory access is always the width of all 32 threads of a warp writing at once their corresponding input element. \\
		This chunk wise algorithm can then be applied in chunks of elements over the entire input data, requiring no reading a bitmask, because the pattern is known beforehand. \\
	
		\section{Popcount}
			The popcount kernel uses a variable chunklength to perform a population count of (enabled) one bits over chunks of the input bitmask. The actual popcount operation is performed on unsigned 32-bit integers using a CUDA intrinsic function. \\
			Each thread processes one chunk of the bitmask, while adjacent threads process adjacent chunks. The output of the popcount ensures large width memory accesses, because all threads store directly next to each other and at once. \\
			
			The writeout phase of the popcount kernel is also a good place to perform a rudimentary histogram creation if required. To distinguish between different distributions of the bitmask with the same $p$ near $0.5$. More on this in section \ref{sec:analysis_writeout} \\
		
		\section{Prefix Sum Scan}
			The exclusive prefix sum as required, denotes for each chunk entry the sum of all entries before it. The exclusive prefix sum does not contain the own entry in the prefix sum for a given entry, so it starts at 0. \\
			
			The prefix sum scan can be split into two phases when parallelized. The up-sweep reduction phase is imagined like a tree such as in figure \ref{fig:pss_up_sweep}. Every entry holds at the end, the inclusive prefix sum of the subtree that reduced to it. Because of the nature of the binary reduction, the inner nodes of the reduction tree correspond to repeating offsets at various powers of two. This is an intermediate product and only becomes useful through some form of down-sweep computation. \\
			The down-sweep phase distributes the values of the up-sweep, through the inner nodes, down into the leafs of the reduction tree, thereby creating the final exclusive prefix sum over all entries. \\
		
			\subsection{Up-Sweep Reduction}
				\begin{figure}[!ht]
					\centering
					\includegraphics[width=0.95\linewidth]{./img/pss_up_sweep.png}
					\caption{\label{fig:pss_up_sweep}Visualization of the up-sweep operation performed by the prefix sum. The left side represents normal operation on a vector with a power of two element count. The right represents adjusted operation to accomodate for non power of two vector sizes. Marked in red is the out of bounds storage location, holding at the end the total amount of one bits. Grayed out cells represent the theoretical power of two cells, no memory is allocated for them.}
				\end{figure}
				One launch of the up-sweep reduction kernel, corresponds to one layer in the prefix sum reduction tree. The kernel is launched $\log_2(\text{total element count})$ times. \\
				To perform one layer of reduction, for every two consecutive tree nodes, their sum is stored in the location of the right node, which will correspond to the left node in the next layer. Growing with the increasing depth of the reduction, towards the root, are the offsets between \emph{adjacent} nodes, i.e. those that need to be summed. Starting at depth zero for the leaf nodes, increasing by one for every layer, the $n$-th left element can be found at index $(2n+1) \cdot 2^d -1$ for depth $d$. The adjacent right element is found $2^d$ elements further to the right. \\
				
				The above algorithm only works on chunk counts equal to a power of two. It can easily be adapted to work with any count of chunks by introducing two more checks. \\
				If both the left and the right subtree indices are out of bounds, their result does not need to be recorded, since it is always zero, and is only going to be propagated further right as well. There would never be any new information introduced that can not be derived from the index, i.e. zero. \\
				There exists at most one pair of subtrees per depth layer, such that the left index exists and the right index is out of bounds. In this case, the value of the left subtree, and the right subtree (zero), can not be stored in the location of the right subtree, and is thus stored in a special out of bounds location, just for this case. Incidentally, after the algorithm is finished on a non power of two chunk count array, or is run again with depth increased by one on a power of two chunk count array, the resulting total number of one bits is stored in the out of bounds location. \\
			
			\subsection{Down-Sweep Collection}
				\label{sec:analysis_pss_down_sweep}
				The down-sweep of the two phase prefix sum algorithm, functions essentially like a reverse reduction, distributing the partial prefix sum values from the inner nodes towards the leaf nodes. This is work efficient, as no operations are carried out multiple times without need. The expansion can be performed in place on the array of chunk entries and results in a concrete per chunk prefix sum value. \\
				Unfortunately this operates on the assumption that every chunks prefix sum is actually necessary, and introduces again only limited use of the massive parallelism the GPU offers. Just like a reduction, many threads would be dormant for most of the time, or if done in layers, there would be many very sparse layers. \\
				
				To circumvent this issue, we devise an algorithm to compute, for a single chunk, its prefix sum value, using only the partial results of the up-sweep phase. \\
				Because this algorithm is, just like the expansion, very memory intensive, even performing it on all chunks individually, though introducing work inefficiency might not draw much of the disadvantages. Adjacent threads will access the same memory most of the time, and the computations are very light. \\
				
				\begin{figure}[!ht]
					\centering
					\includegraphics[width=0.45\linewidth]{./img/pss_pssidx.png}
					\caption{\label{fig:pss_pssidx}Visualization of the single chunk down-sweep calculation, performed for the element highlighted with a blue arrow. Below the partial prefix sum buffer is a virtual representation of the tree that lead to it. Green boxes in the tree mark accepted (fitting) sub trees, red boxes mark rejected ones. The vertical blue lines ends the range into which sub trees are fit. \\ Note that the rejected red subtree is the \emph{only} rejected subtree, no subtree without a box in the figure was considered, as described in the algorithm.}
				\end{figure}
			
				The algorithm essentially tries to fit as many, as large as possible, left sided subtrees into the index of the chunk to compute. Inputs are the chunk index, the partial prefix sum array and half of the smallest power of two that is greater or equal to the chunk count. The working operation is described in the algorithm \ref{alg:pssidx}. \\
				
				\begin{algorithm}[!ht]
					\SetAlgoLined
					\KwIn{cid, id of the target chunk}
					\KwIn{p2, half of the smallest power of two that is greater of equal to the chunk count}
					acc $\gets 0$; \\
					consumed $\gets 0$; \\
					\While{p2 $>= 1$}{
						\If{cid $>= consumed + p2$}{
							add value of prefix sum subtree at [consumed$+$p2$-1$] to acc; \\
							add p2 to consumed; \\
						}
						divide p2 by 2; \\
					}
					\KwOut{acc, prefix sum of the target chunk}
					\caption{\label{alg:pssidx}On-the-fly Prefix Sum Computation}
				\end{algorithm}
			
				This also works on non power of two chunk counts, without adjustments. \\
				The power of two that is divided every iteration (depth level), can be imagined as a subtree that has at its leaf level a width of p2. The colored boxes in figure \ref{fig:pss_pssidx} represent the leaf level width (leaf length) of the subtrees being fit. The boxes are placed at the highest node of their respective subtree, emphasizing the focus on fitting the largest possible subtrees first, as may include smaller subtrees that then do not need testing. \\
			
		\section{Writeout}
			\label{sec:analysis_writeout}
			A naive approach to perform the writeout of enabled elements to the output, would be to let adjacent threads work on seperate (possibly adjacent) chunks of input data in parallel. This does not however accomodate the fact that larger width (coalesced) memory accesses perform generally much better on GPUs. The pipeline queue for the LSUs might quickly be filled and lead to warps actually stalling the SM. \\
			
			Alternative to the chunks per thread approach, there is a chunk per threads approach. Here one chunk of 1024 input elements are processed at once, by one full warp of threads. There is no inter warp-synchronization required, as all thread-to-thread communication is only performed intra-warp. \\
			The algorithm chosen starts by loading 32-bit long parts of the bitmask, one per thread of the warp, into shared memory. This makes up 1024 total bits loaded, and explains the fixation on 1024 input elements processed per chunk. \\
			To coalesce memory accesses into larger widths, for greater GPU memory performance, the threads of a warp collectively process steps of 32 input elements at once, as many as are enabled. Unfortunately this may lead to half a dormant warp per (up to) 32 element writeout, however all threads store adjacent to each other and at once, and can as such be grouped up to fewer, larger width memory transactions. \\
			To determine wether or not a thread has to participate in a given 32 element processing step every thread can use the bitmask stored in shared memory, to determine if the bit corresponding to its position is enabled. This leaves open the issue of what position in the output a certain thread takes when writing out its element. It can be solved with a population count of one bits, over all bits positionally before the bit corresponding to the thread. I.e. elements before the own in the 32 element step, which is again an exclusive prefix sum. \\
			It is not necessary to calculate the prefix sum for all of the chunks processed by a warp. If the chunks are arranged in a continuous fashion, only the prefix sum of the first chunk is required. \\
			
			As mentioned above, the prefix sum for a chunk of elements to process may not always be actually required. For optimization of very sparse bitmasks, a secondary popcount in a seperate buffer may be useful, as processing of a chunk may be entirely skipped if the popcount entry in that chunk is zero anyway, i.e. there are no elements to process. \\
			Because performing another popcount kernel pass is not free, it is useful that the total amount of one bits set is already known after the prefix sum up-sweep. This information can be used to determine if a second popcount pass would gain any/enough performance benefits to justify its cost. \\
			Without using a histogram on chunks, for non uniformly distributed bitmasks like a zipf or burst distribution, the amount of total one bits may not be enough to immediately tell that the optimization targeted popcount pass would be worth it. \\
			
			For repeating pattern bitmasks, writeout of the input elements can be optimized to always use all 32 parallel threads of a warp with adjacent writeout locations to accomodate larger width memory transactions. There are no dormant threads on a 32 element writeout threads, except possibly at the end of a chunk. \\
			To achieve this behaviour, the threads of a warp need to process 32 elements which will be adjacent to each other in the output at once per step. After calculating the position of the first one bit per thread, they use a 32 element looup table that can be precomputed before the kernel launch on the CPU. The table denotes for every one bit, the amount of indices to skip such that 31 intermediate one bits have been skipped over. Using the intra-chunk writeout offset modulo the pattern length returns the entry to to check in the table, the value of which is added to the offset. \\
		
		\section{Streaming}
			In the algorithm implemented through the above mentioned kernels, there exists a dependency of the writeout kernel on the prefix sum scan, which in turn requires values from the popcount kernel before being able to do its work. This dependency is however, only really true for the current, and all preceeding chunks, i.e. a writeout of a chunk somewhere in the middle of all elements, does not depend on the prefix sum of chunks behind it, or their popcount. It is merely required that all chunks up to and including itself have a finished prefix sum. \\
			Because for some small total element counts, the popcount and prefix sum kernels might not be enough to saturate the entire device, one approach is to use streaming to start earlier with the more expensive writeout of chunks whoose prefix sum is already finished. \\
			This way, the theoretical maximum of time that can be saved is the runtime of almost the entire popcount and prefix sum kernel. For large element counts however, there is no time saved because the popcount and prefix sum kernel themselves would be enough to saturate the device. The same approach could also be used to hide data transfer from the CPU to the GPU, by already starting computation on transferred chunks. \\
			
			To avoid synchronization between the CPU and GPU, the existing dependency of kernels can be modeled using CUDA events, they enable efficient synchronization on the GPU itself. This way the writeout kernel for a chunk is immeditaly scheduled when the corresponding prefix sum kernel finishes. \\
		
		\section{Summary}
			We have seen that the conmpressstore operation can be split into three dependant tasks, i.e. the popcount, the prefix sum scan and the writeout. Workload can be made easier for the intermediate steps by introducing the concept of chunks, blocks of consecutive input data that are always processed together. \\
			The workings of a two phase prefix sum scan are also explained, providing oppertunity to skip the second pass and instead calculate it on the fly per chunk. \\
			Acceleration of sparse bitmasks has also been considered. For the general case, two approaches to the writeout task are described, aiming for performance imporvements via coalescing memory accesses. \\
		
	\chapter{Evaluation}
	
		\section{Experimental Setup}
			All implementations have been tested on a RTX Quadro 8000 GPU, installed on a server provided by the chair of database theory, running a linux operating system. This GPU has 72 SMs and 48GB of GDDR6 global memory with a theoretical bandwidth of about 672 GB/s \cite{quadrortx8000_specs}. All tested algorithms used 64-bit integers as benchmark data to perform the compressstore on. \\
			
			To individually test the developed kernels, ignoring any PCIe data transfer times, CUDA events were used. Placing one event just before, and another just after the kernel launch, allows precise timing of runtime on the GPU. Where possible, grid- and block-dimensions as well as chunklength were varied. The tested kernels are:\\
			\begin{itemize}
				\item Popcount kernel \\ (\texttt{3pass\_popc\_none})
				\item Partial prefix sum kernel over all required depths, using global memory \\ (\texttt{3pass\_pss\_gmem})
				\item Per chunk prefix sum completion, using global memory \\ (\texttt{3pass\_pss2\_gmem})
				\item Naive chunks per thread writeout kernel, using \textbf{p}artial and \textbf{f}ull prefix sum buffers \\ (\texttt{3pass\_\textbf{p}proc\_none} and \texttt{3pass\_\textbf{f}proc\_none})
				\item Optimized chunk per threads writeout kernel, using \textbf{p}artial and \textbf{f}ull prefix sum buffers \\ (\texttt{3pass\_\textbf{p}proc\_true} and \texttt{3pass\_\textbf{f}proc\_true})
			\end{itemize}
			In all graphs where not otherwise noted, the best performing grid-/block-dimensioning and chunklength are represented. \\
			
			To provide a baseline comparison for the developed kernels, both CPU and GPU based existing approaches are tested:
			\begin{itemize}
				\item CPU based singlethread \\ (\texttt{cpu})
				\item CPU AVX base singlethread \\ (\texttt{avx512})
				\item GPU based singlethread \\ (\texttt{single\_thread})
				\item GPU based CUB flagged operation, using a bytemask \\ (\texttt{cub\_flagged\_bytemask})
				\item GPU based CUB flagged operation, using an iterator over a bitmask \\ (\texttt{cub\_flagged\_biterator})
				\item GPU based CUB prefix sum scan \\ (\texttt{cub\_pss})
			\end{itemize}
			
			To provide a more complete comparison against the baselines and each other, some graphs use \emph{packages}, i.e. grouping runtimes of kernels as they would be used in a proper scenario to perform the compressstore operation. The packages represented are:
			\begin{itemize}
				\item Streaming using the best stream count and best in class grid/block/chunk-dimensioning \\ (\texttt{async\_streaming\_3pass})
				\item Stand-alone processing using the optimized writeout kernel and two phase prefix sum calculation \\ (\texttt{3pass\_fproc\_true\_sa})
					\subitem = 2 * \texttt{3pass\_popc\_none} + \texttt{3pass\_pss\_gmem} + \texttt{3pass\_pss2\_gmem} + \texttt{3pass\_fproc\_true}
				\item Processing using the optimized writeout kernel and a partial prefix sum \\ (\texttt{3pass\_pproc\_true\_sa})
					\subitem = \texttt{3pass\_popc\_none} + \texttt{3pass\_pss\_gmem} + \texttt{3pass\_popc\_none} + \texttt{3pass\_fproc\_true}
				\item Processing using the optimized writeout kernel and a full prefix sum provided by CUB \\ (\texttt{3pass\_fproc\_true\_cub})
					\subitem = \texttt{3pass\_popc\_none} + \texttt{cub\_pss} + \texttt{3pass\_popc\_none} + \texttt{3pass\_fproc\_true}
			\end{itemize}
	
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=0.9\linewidth]{./img/runtime_over_datasize_normal.png}
			\caption{\label{fig:runtime_over_datasize_normal}Logarithmic runtime of all tested components over increasing datasize. Fixed $p=0.5$ at a chunklength of 1024 where applicable.}
		\end{figure}
	
		Both CPU based approaches, as well as the GPU based \texttt{single\_thread}, performed at all times worse than all other kernels that properly utilize the GPU's parallelism. \\
	
		\section{Popcount}
			\label{sec:evaluation_popcount}
			The popcount kernel takes up relatively little runtime in the total compressstore operation. Because there is no comparison for it and the performed actions are simple, the runtime can be considered almost negligable. \\
			The memory access pattern for the chunkwise writeout of the popcount result deliberately accomodates for the GPUs preferrence towards larger width load/store transactions. \\
			
			Measurements show that a minimal modification of the algorithm, to utilize a 64-bit integer popc instruction, almost halves the runtime. To preserve compatibility with 32 bit long chunks, additional guards have to be inserted for the edge cases. Across 8GiB of input data, this is still only an improvement of roughly $0.25\text{ms}$ on the provided hardware. \\
			
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/runtime_heatmap_dimensioning_3pass_popc_none.png}
				\caption{\label{fig:runtime_heatmap_dimensioning_3pass_popc_none}Runtime heatmap of various block- / grid-dimensions for the popcount kernel. Fixed at 8GiB of data and $p=0.5$ with a chunksize of 1024 bits.}
			\end{figure}
			
			The heatmap \ref{fig:runtime_heatmap_dimensioning_3pass_popc_none} shows the runtime of the popcount kernel in ms, varying over some powers of two for both grid- and block-dimensioning. A slight saturation wave pattern is visible, showing that there is an optimal range of threads per SM at which the algorithm performs best. The pattern is visible as a diagonal line because both axes are logarithmic with the base two. \\
			Using less block and/or threads per block degrades performance substantially, as the device can not be saturated, and latencies can not be hidden. Using too high dimensioning the performance drops again, even if very subtly, due to increased scheduling effort with no benefit. \\
			The most interesting observation to be made is the relatively large drop in performance when using 1024 threads per block instead of a lower power of two. The Nsight Compute report showed the difference between these to be an almost 60\% worse L1 cache hit rate from 1024 threads per block version compared to the 512 dimensioning. The dramatically decreased hit rate also caused the LSU transaction queue to fill more frequently, stalling the SM for much longer, leading to yet again decreased performance. \\
			
			\subparagraph{Further experimentation} on this kernel may be targeted towards increasing the usage efficiency of the LSU. \\
			In its current form the popcount kernel loads for every thread, data from a different chunk. With older GPUs this results in the LSU loading larger widths of data than are actually required. In the best case, this extra loaded data could then be reused from cache when each thread gets to processing its next few elements. Unfortunately GPU caches, especially the L1 caches inside the SMs are very small, especially when compared to their CPU counterparts. Due to the low computational requirements of each kernel, lots of threads will be alive per SM. This will likely cause many L1 cache misses, leading to even more unneccessary loads. \\
			On more modern GPUs, the LSUs may perform smaller width loads of down to only 32 bytes each. This may alleviate some of the cache issues, however due to the many threads alive, the LSU's queue of transactions can quickly fill up and cause actual stalling of the SM, while threads wait for the queue to empty again. \\
			One way to reduce the impact of this issue would be to perform adjecent loads using all 32 threads of a warp, and process consecutive chunks by sliding the window of popcounting threads over them. The little synchronization required to collect the results from every thread can easily be hidden by the many memory transactions of other threads. \\
		
		\section{Prefix Sum Scan}
			Just like the popcount kernel, the two phase prefix sum scan, and especially the up-sweep phase take up very little of the total runtime of the compressstore operation. Additionally, both phases work only on the prefix sum buffer, which only has one entry per chunk. As chunklength increases, the amount of data touched to perform the prefix sum drop dramatically. Both phases work solely on global memory. \\
		
			As visible in the later shown figures \ref{fig:package_runtime_over_p} and \ref{fig:package_throughput_over_datasize_normal} there is barely any difference between \texttt{3pass} packages using the stand-alone prefix sum versus that calculated by cub. This also extends to the use of the partial prefix sum. \\
		
			\subsection{Up-Sweep Reduction}
				Due to the large offset and stride of the elements accessed by each thread, there is no pattern to coalesce memory transactions into, resulting in lots of unneccessary data loaded. This kernel is again very memory bound by the many transactions performed, as there is also very little computation happening. \\
				This issue is usually, and most commonly for reductions, weakened in impact by using shared memory. This would cluster kernel runs for a certain depth together into one run, and every block then needs only load the data once at the beginning and store once at the end. All intermediate depths compute on the part of the prefix sum buffer available in shared memory, at the single cycle per access cost. \\
				Because of the very low runtime part this has in an overall package already, this optimization was not investigated further. \\
			 
			\subsection{Down-Sweep Collection}
				The down-sweep phase of the prefix sum uses the special method explained in section \ref{sec:analysis_pss_down_sweep} to compute a per chunk final prefix sum, using the partial data from the up-sweep phase. The writeout pattern of the prefix sum result per chunk is as large as possible, accomodating again, the GPUs prefference towards larger width memory transactions. \\
				Using this approach the data loaded to compute the prefix sum for every chunk introduces uneccessarily loaded bytes. However, the result of a load can, because adjacent threads process adjacent chunks, almost always be used by an entire warp of threads simultaneously. In the best case, other threads of the same block may reuse the data from the L1 cache. This is because for many of the depth iterations in the algorithm, the sub trees fitted into the target chunk position are shared for all of the threads chunks. \\
				
				This kernel touches even less data then the up-sweep phase, and has an even lower runtime. Even though both phases operate only on global memory, any gains by modifying them to use shared memory, are negligible due to their low runtime contribution to the total package.
		
		\section{Writeout}
			The writeout kernels take up by far the largest amount of runtime of any compressstore package. Considering the very small impact of both the popcount and prefix sum kernels, this section compares packages against each other.
		
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/package_runtime_over_p.png}
				\caption{\label{fig:package_runtime_over_p}Runtime of some compressstore packages over varying values of $p$, fixed at 8GiB and 1024 bit chunklength where applicable.}
			\end{figure}
			
			As mentioned in the analysis of the writeout kernel, the naive chunks per thread implementation has a much worse memory access pattern than the optimized chunk per threads kernel. The measured impact of this is a roughly 10 fold increase in runtime. No package using the naive approach is shown in figure \ref{fig:package_runtime_over_p}, as it would only degrade the ability to compare the optimized version with CUB. \\
			
			The figure \ref{fig:package_runtime_over_p} clearly shows the 1024 zero bit optimization working very well to drastically reduce the runtime for lower $p$ values. In the more dense ranges of $0.5$ and upward, the CUB algorithm performs slightly more favorably, taking only a few milliseconds less runtime. \\
			
			Between the stand-alone prefix sum and using the optimized CUB functionality, there is barely a noticable difference in package runtime. This holds true for full and partial processing, the latter saving in theory a few more calculations and memory loads. \\
			
			Regarding $p$ values around $0.5$ and higher, performance for the \texttt{3pass\_proc\_true} packages is very near that of CUB. One possible reason for the developed algorithm to still perform worse may lie in the fact that there are at many times, especially near the $p=0.5$ value, many dormant threads in a warp per writeout pass. This is caused by the internal no synchronization principle of the algorithm, as explained in section \ref{sec:analysis_writeout}. \\
			One possible way to avoid this would be the precomputation of all writeout positions using the bitmask from shared memory, requiring another 1-2KiB of register/shared memory storage per warp of a block. The increased maximum width writeout size for every warp writeout cycle may clear space in the LSU transaction queue and reduce related stalls. Additionally the precomputation of the writeout positions can provide oppertunity to hide latency of other memory operations, as otherwise this kernel is very memory bound and stalls dozens of cycles between issuing instructions. \\
			This optimization could give rise to another potential saving. If the whole bitmask of a chunk to precompute is loaded into shared memory, then it can also directly be used to much detect much finer empty and full bitmask patterns. Instead of investing a whole popcount run just to detect empty chunks, the precomputation step includes this at essentially no extra cost. \\
			
			\begin{table}[!ht]
				\centering
				\begin{tabular}{c | c c c}
					\hline
					Algorithm & Uniform ($p=0.50$) & Burst ($p=0.50$) & Zipf ($p=0.29$) \\
					\hline
					3pass & $26.58$ & $18.90$ & $19.76$ \\
					CUB & $23.50$ & $23.78$ & $21.59$ \\
					\hline
				\end{tabular}
				\caption{\label{table:masktype_perf_comp}Runtime in milliseconds of \texttt{3pass\_fproc\_true\_cub} with its best-in-class configuration and \texttt{cub\_flagged\_biterator}, across three different masktypes. The given $p$ value of each masktype tells the total portion of one bits across the 8GiB dataset for the particular distribution.}
			\end{table}
			
			Optimizing runtime for 1024 bit long zero streaks works well on uniformly distributed bitmasks with very low $p$ values. However the $p$ value alone can only tell when this optimization will definitely improve performance, not rule out potential in itself. In the table \ref{table:masktype_perf_comp} the runtime values for zipf and burst type bitmasks are given for the best performing CUB baseline, and the best performing \texttt{3pass} implementation. \\
			Both the zipf and burst bitmask have a very high $p$ value, but they are not uniformly distributed, so they still score very high numbers of skippable 1024 zero bit streaks. \\
			Performance for the burst mask is slightly higher than the zipf mask. Although this is implementation specific, the tested GPU schedules blocks with lower indices earlier, causing almost all non-skippable chunks to be processed first for the zipf mask and leaving almost only empty chunks leftover, which take a lot more computational resources than memory loads, due to the optimization. The burst mask does not always suffer this problem, due to the very long passages of only ones or only zeroes being distributed somewhat evenly across all chunks. This ensures there is always memory intensive writeout work, and computation focused skipping happening. \\
		
		\section{Streaming}
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/package_throughput_over_datasize_normal.png}
				\caption{\label{fig:package_throughput_over_datasize_normal}Throughput of some compressstore packages over increasing datasize, fixed at $p=0.5$ and 1024 bit chunklength where applicable.}
			\end{figure}
			
			The streaming compressstore package performs worse than any of the packages using the optimized writeout kernel. Because it itself profits from this kernel, on very low $p$ values, streaming still runs quicker than CUB. \\
			The figure \ref{fig:package_throughput_over_datasize_normal} shows that the streaming approach only starts maximizing its potential at around 1GiB of total data size. Various powers of two have been tried for the stream counts, every data point represents the best performing stream count at that position. \\
			
			The large performance loss for small datasizes is likely owing to the extra synchronization required to launch the kernels seperately on many streams. The CUDA events are placed in a way that they can be recorded and waited for on the GPU using no additonal host side processing. The small datasize of every streams portion of the total data causes large portions of the devices processing and memory power to remain unused. Most of the runtime is supposed to be used up during writeout, which has to synchronize with previous prefix sums. Per small portion alone there is just not enough work available to saturate the devices capabilities. \\
		
		\section{Summary}
			Overall the performance of the new hand crafted kernels matches that of CUB, the best baseline approach, very nearly in many cases. In tests involving very sparse bitmasks, or masks with ranges of low density, the performance of the developed packages is several times greater than that of CUB. \\
			All CPU and GPU based single thread approaches perform much worse than both CUB or \texttt{3pass}. \\
			
			Some problematic areas of the \texttt{3pass} kernels have been identified, and possible starting points for future research are provided. \\
		
	\chapter{Conclusion}
		As noted in the evaluation the \texttt{3pass} kernels developed in this work perform best on inputs with bitmasks that contain areas of extreme sparcity. This feature does not have to be on a global level, but easy detection through a count of selected elements is not given on local levels only. Working on these sparse bitmasks enables the created kernels to perform the compressstore operation in a faster runtime than any of the baselines. The improvement over singlethreaded CPU and GPU based approaches is present in the general uniformly distributed $p=0.5$ case as well, even though compared to the CUB baseline the new kernels perform slightly worse for these denser masks. \\
		
		\paragraph{Related Work} Apart from the existence of CUB, that provides compressstore functionality using the GPU, there seems to be relatively little related work regarding this topic. Particularly pertaining to GPU acceleration of this issue, we have found none other accessible at the time of writing. Nevertheless, sporadic mention of the algorithm exists. \\
		Already mentioned in the introduction was the use of the AVX-512 compressstore instruction for use in reducing problem sizes for subsequent steps in a high throughput detector employed at CERN \cite{cern_datastreaming}. This problem size reducing property is also particularly important for any GPU based database processing. As PCIe transfer is quite slow, data on the GPU should be used in the most efficient way possible, performing early projection operations using a compressstore instruction can save valuable space for parallel processing of transactions. \\
		The Template Vector Library described in the work of \citeauthor{tvl} \cite{tvl} describes an approach to database operations that is hardware oblivious. This abstraction layer is complemented by hardware conscious plug-ins, which can then be optimized much more for their respective hardware, than a general approach permits. In a theoretical CUDA plug-in to the Template Vector Library, either CUB or \texttt{3pass} might be up for selection when issuing the operation, depending on the situation. This decision could be made using information potentially available before the compressstore operation is performed, through previous processing steps. \\
		As to in how far these can be considered related work is up to the reader to decide. \\

		\paragraph{Future Work} The evaluation section already mentioned some possible approaches to further increase throughput of the kernels used in \texttt{3pass}. Specifically for those regarding the first two preperation passes, popcount and prefix sum creation, the fraction of the total runtime is so small that most improvements would yield little impact. Nevertheless for use cases where many smaller compressstore utilizing transactions are performed in parallel on the same GPU, these optimization might quickly add up. \\
		As explained in section \ref{sec:evaluation_popcount} when adopting a more rigid chunk length sizing, 64-bit popcount instructions prove almost twice as fast as 32-bits. In the usual vein of optimizing memory accesses towards broader transactions, the per warp loads performed on the bitmask might benefit from being adjacent, relying less on the cache and giving more space to the LSUs. We do not think the extra synchronization required to perform the popcount of adjacently loaded bitmask data will be noticable. \\
		To improve memory efficiency for the prefix sum calculation as well, shared memory may be used to accelerate the reduction into less kernel launches. At the same time as the reduction, would be a good time to perform a histogram creation for some rudimentary types of mask chunks. The histogram would give a better understanding of the density distribution of selected elements in the mask. This could be used to make a more informed decision wether to use CUB or \texttt{3pass} for the operation, before the bulk of the writeout work actually has to be invested. \\
		Aside from the mentioned static pattern optimization through a table of skips in the mask, the general case could benefit from a similar approach of keeping the whole 32 threads of a warp alive at writeout time. Precomputation of this extra information required is possible directly in the writeout phase, using data already available in shared memory. If the same stage could replace the second popcount needed for long 0 bit streak optimization is another thing in need of measurement. \\
		
	\chapter*{Bibliography}
		\printbibliography[heading=none]
	
\end{document}
