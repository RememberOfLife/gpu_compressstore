%\documentclass{tudscrartcl}
\documentclass{tudscrreprt}
%\documentclass[ngerman, 11pt, a4paper, twoside, cleardoublepage=empty, open=right, numbers=noenddot, cd=lightcolor, final]{tudscrreprt}

% fix bib
\usepackage[style=numeric,sorting=nyt]{biblatex}
\addbibresource{main.bib}

\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}

% for cpp
\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{microtype}

% review highlighted text
\usepackage{xcolor}
\definecolor{review}{RGB}{221,33,114}
\newcommand{\markr}[1]{\textcolor{review}{$\langle$#1$\rangle$}}

\usepackage{tikz}

\begin{document}
	
	\faculty{Fakultät Informatik}
	\department{Institut für Systemarchitektur}
	\chair{Lehrstuhl für Datenbanken}
	\date{15.06.2021}
	\title{Tailoring Compressstore to GPU}
	\thesis{bachelor} \graduation[BSc.-Inf.]{Bachelor-Informatik}
	\author{ Urs Kober
		\matriculationnumber{---}
		\dateofbirth{DD.MM.YYYY}
		\placeofbirth{---}
	}
	\matriculationyear{2018}
	\supervisor{Prof. Dr.-Ing. habil. Dirk Habich \and Dipl.-Inf. Johannes Fett}
	\professor{Prof. Dr.-Ing. habil. Wolfgang Lehner}
	\maketitle
	
	%\confirmation
	%include screenshot confirmation with signature here
	\vspace*{5cm}
	\includegraphics[width=\linewidth]{img/confirmation.png}
	\thispagestyle{empty}
	\newpage
	
	\chapter*{Abstract}
		In this work, we introduce a new set of kernels to perform the compressstore operation on the GPU. We analyse the problem and devise an algorithm capable of parallelizing the issue. Along the way we present some considerations that were made in order to increase performance, and evaluate their usefulness. The results show that, compared to the best existing baseline we found, CUB, our new \texttt{3pass} kernels achieved runtimes slightly slower than those of CUB in the general case. However, in some of the special cases we looked at, such as bitmasks with extreme sparsity, our kernels perform considerably better than CUB.
	
	\tableofcontents
	
	\chapter{Introduction}
		With the rise of increasingly capable computing hardware and often even faster growing data collection mechanisms, modern data refinement, too, is becoming more complex and computationally expensive. Recent years have shown a trend of using general purpose graphics processing units (GPUs or GPGPUs) to accelerate traditional easily parallelizable workloads such as these, and database systems in general \cite{gpgpu_sql, gpgpu_db, gpgpu_cuda}. \\
		
		This work deals with fitting the compressstore operation to the GPU. The compressstore operation also has applications in the mentioned data refinement workflows. \\
		Most obvious is the use as a database projection operation, selecting only some columns of data from a larger set. This non-general operational mode of a compressstore can be optimized for every given pattern at runtime. \\
		A more general use might be to reduce the size of a problem between single steps of a process. This may be after certain filters have been applied to an intermediate dataset and excess data is best discarded to reduce unnecessary workload on successive processing steps. When the data to remove is not partitioned in a regular fashion the issue calls for the compressstore operation. \\
		Similar use, although on a different granularity, is required at enormous data collection facilities such as a CERN detector \cite{cern_datastreaming}. \\
		
		Viability of the compresstore's ability to save even more time overall relies heavily on fast runtimes of the procedure itself. Catering to this need is the massively parallel computing capability of modern GPUs, which can be utilized to a great extent with the compressstore operation. This parallelization requires careful tuning of GPU specific parameters to realize the highest performance gains. \\
		
		Regarding database uses, the input masks used for the operation may not always be uniformly distributed. Some of these non-uniform masktypes may contain even more potential for optimization. \\
		
	\chapter{Foundations}
		In this section we will introduce key concepts required to understand the work and function of the kernels. Specifically we start by defining the compressstore problem and its nuances, before providing a broad overview of the Nvidia CUDA GPU architecture and execution model as it was used to develop and compare the implementations. For usage as baseline comparisons we also briefly look at AVX and the existing CUB implementation.
	
		\section{Compressstore}
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/compressstore_operation.png}
				\caption{\label{fig:compressstore_operation}Visualized compressstore operation on a 16 element long data vector and bitmask. Any values of "Don't Care" elements in the output are not relevant to the operation. The output represents only those elements from the input for which there was an enabled bit corresponding in the mask (marked green). There are no spaces of masked out (marked red) elements between the output elements.}
			\end{figure}
			The compressstore operation is used to store certain elements of an input vector contiguously in memory, see figure \ref{fig:compressstore_operation}. Using the provided bitmask the operation only selects elements for writeout that have a corresponding enabled bit (set to one) in the bitmask. Potential gaps between enabled elements are removed, thereby left packing the results into the output. The length of the output vector will always be less than or equal to that of the input vector. \\
			Even though not all applications of this operation require the same assumptions, in this work a few common ones are made.
			\begin{itemize}
				\item The input vector of elements may not be overwritten.
				\item The order of selected elements is kept stable through the writeout process.
				\item There is no information about the bitmask in advance.
				\item We do not care about the contents of output elements that were not selected.
			\end{itemize}
			Parallelization of this operation is not entirely trivial due to every input element requiring the writeout location in the output vector as a count of the previous enabled inputs, before processing can occur. \\
			Depending on the density and distribution of bits set to one in the bitmask there are various special cases, some of which may benefit from specific acceleration. \\
			These include but are not limited to:
			\begin{itemize}
				\item very sparse or dense bitmasks (with uniform distribution)
				\item zipf distributed bitmasks as they might spawn from an ordered group by query filter
				\item burst masks as they might spawn from a tablescan over data with temporal dependency
				\item or repeating pattern bitmasks, useful for performing a project operation.
			\end{itemize}
			All of which will see discussion in this work. \\
			The overall density of a bitmask in terms of a ratio $\frac{\text{one bit count}}{\text{total element count}}$ will be called $p$ from here on. It should be noted that this does not give any indication as to what distribution is present in the bitmask. A bitmask where the entirety of the second half of the bitmask is set to one bits and the first half set to zeroes, will still yield $p=0.5$ just like a uniform random distribution might. \\
		
		\section{GPU Computing}
			\subsection{Architecture}
				GPUs have, as their name implies, been originally devised as a dedicated device for graphical computation workloads, especially in realtime scenarios \cite{cuda_arch}. They have since their inception also seen much use as accelerators for general purpose computing workloads, many of which benefit greatly from the high parallelism they provide. \\
				Modern GPU architecture consists, across different hardware manufacturers, of mostly differently named but functionally somewhat similar components \cite{nvidia_vs_amd_arch}. The basics of the CUDA architecture are described here, in order to give a brief overview of the structures available to improve performance on NVIDIA GPUs. \\
				
				\begin{figure}[!ht]
					\centering
					\includegraphics[width=0.95\linewidth]{./img/gpu_architecture.png}
					\caption{\label{fig:gpu_architecture}Simplified GPU architecture diagram. In reality the relative position and size of many components frequently changes with each generation. Some parts are omitted for clarity. The actual number of cuda cores (marked green) per streaming multiprocessor (SM) is higher in real devices. All cells marked green are cuda cores, only one is labeled. The yellow cells with the label LSU show the load/storeunits responsible for memory accesses outside of the SM. Labeled SFU are the special function units capable of computing more complex math instructions using extra hardware acceleration.}
				\end{figure}
				
				The compute unified device architecture (CUDA) of all recent Nvidia GPUs allows generalisations about the design of the GPUs processing power, helping developers to easier target multiple devices with fine tuned applications. \cite{cuda_smcc} \\
				The concepts covered in this section are also explained in the Fermi Generation Whitepaper from NVIDIA \cite{fermi_whitepaper}, and one of many publicly available university lectures on the topic \cite{gpu_lecture}. Additionally the very comprehensive CUDA programming guide from NVIDIA \cite{cuda_programmingguide} covers many more details that did not fit in this brief overview of the NVIDIA GPU architecture and execution model. \\
				
				Similar to the RAM of a processor, a typical Nvidia GPU has a global memory shared among all of its processing power. This is also used by device intrinsic management functions. Between interactions of processing elements and the global memory there exists an on-chip L2 cache and multiple memory controllers to handle the transactions. \\
				Communication with the host is performed through the PCIe host interface. \\
				
				When executing code on the GPU, several parallel and concurrent threads all execute the same code. Discernible difference for accessing inputs and outputs is provided solely through a running count across all threads of a workload. More on this and the use of branches in section \ref{sec:gpu_execution_model}. \\
				When launching a kernel, the workload that is meant to run on the GPU, the user specifies a number of blocks (grid dimension) and a number of threads per block (block dimension) for the GPU to launch. \\
				Scheduling of blocks on the GPU is managed by the GigaThread engine, specially designed by NVIDIA to handle the immensely parallel tasks assigned to GPUs. It distributes the outstanding blocks of running kernels onto the many streaming multiprocessors (SMs) of the GPU. The SMs are the key structure of processing, of which several dozens can be found in modern devices. When a block of threads is assigned to an SM, the threads it contains are grouped into warps of 32 each. \\
				
				Each SM contains many so called CUDA cores, which are the real execution units used by warps running on the SM. Every thread alive on an SM has its own registers in a very large register file that serves the entire SM. To distribute the work that threads want to perform across the CUDA cores, the SM uses several warp schedulers that assign a whole warp of threads to execution units respective of the instruction that is being executed. \\
				Because every thread has effectively its own register file, there is a zero overhead scheduling of warps in every SM. As long as there are warps available to schedule, i.e. warps that have instructions for which there are currently available execution units in the SM, the warp schedulers issue the warps to their execution units between cycles, such that in every cycle actual work gets done. \\
				
				The CUDA cores available in every SM vary across different GPUs and GPU architecture generations, but in general there are many integer and floating point arithmetic units available, with a small number of specialized function units (SFU) available for computing tensor or raytracing operations as well as several hardware accelerated math functions. \\
				
				Memory accesses are grouped together into larger width transactions where possible, and performed and processed by the specialized load/store units. There exists a small L1 cache for global memory operations, which shares a configurable amount of space with the per SM shared memory. Threads on the SM can read from and write to shared memory effectively in only one cycle, however there are serialization limitations if multiple threads perform writes on the same location. \\
				Memory performance intensive applications that issue a lot of read/write instructions to the load store units (LSU) may quickly suffer of warps stalling due to a reached pipeline limit for the amount of operations in transit per LSU. This applies especially if the widths of memory accesses are smaller, which leads to less coalesced memory accesses and therefore more individual transactions. \cite{cuda_memorycoalescing} \\
			
			\subsection{Execution Model}
				\label{sec:gpu_execution_model}
				As touched on in the architecture section, all threads of a kernel execute the same code. In the case of CUDA this is written using a C++ extension and the nvcc compiler provided by Nvidia. Such a function, that is executed by all threads, is called a kernel. To create differentiation between threads, intrinsic values for the block number/dimension and thread number inside a block can be used together to create a global thread identification. \\
				The grid and block dimensioning is not limited to one dimension but each can, seperately, be up to three dimensional. \\
				
				Instructions are, usually, excluding certain cases on newer devices, executed in lockstep for the entire warp at once. The single instruction multiple threads (SIMT) model of execution functions as a natural, more parallel, progression from single instruction multiple data (SIMD) as it is used on many CPUs. \cite{simd_simt_smd} \\
				The SIMT model allows actual branching of code, contrary to the SIMD approach of only masking out operations of disabled elements during a vector operation. Branches in the code are serialized through branch predication, and thus very time intensive. All threads not participating in a given branch effectively perform NOPs while the other threads execute. In excess this behaviour is called warp (execution) divergence and can cause large performance losses. \\
				
				Kernel launches and other CUDA library functions, such as copying memory between the host and device may also accept a stream argument. By default these operations are serialized in the default stream. Using non-default streams they can be parallelized on the GPU, and furthermore, be partially synchronized using events that are triggered and waited for by streams. \\
				Using streams enables partial processing of data while still copying the rest from the host side to the device, as well as many other applications of parallel kernel execution on the device. When using events this enables dependencies between larger chains of kernel launches and device functions. \cite{cuda_streams_devblog, cuda_streams_events} \\
				
				Minimal exemplary code for an increment kernel is provided in the listings \ref{lst:host} and \ref{lst:device}. For further information refer to the CUDA programming guide \cite{cuda_programmingguide}. \\
				\newpage % \markr{careful here this is terrible formatting}
				\begin{lstlisting}[label=lst:host,caption=Device Side: Increment Kernel]
__global__ void increment_kernel(int* data, int N) {
	int tid = threadIdx.x + blockIdx.x * blockDim.x;
	if (tid < N) {
		data[tid] += 1;
	}
}
				\end{lstlisting}
				
				\begin{lstlisting}[label=lst:device,caption=Host Side: Device Malloc and Kernel Launch]
int* h_data = (int*)malloc(1000*sizeof(int));
// ... h_data gets populated with numbers
int* d_data; // pointer to data on the device
cudaMalloc(&d_data, 1000*sizeof(int));
cudaMemcpy(d_data, h_data, 1000*sizeof(int), cudaMemcpyHostToDevice);
increment_kernel<<<4, 256>>>(d_data, 1000);
				\end{lstlisting}
				
		\section{AVX}
			Advanced Vector Extensions (AVX) are SIMD extensions to the x86 instruction set architecture supported by many modern CPUs. AVX instructions are essentially hardware acceleration features for operations that otherwise require multiple non-AVX instructions. The idea of this vector processing is to execute a given operation on all elements of a fixed size vector at once. Larger problems can usually be broken down into smaller vectors that can be processed by AVX and similar vector processing instructions. \cite{avx} \\
			Data for these operations has to be provided in special AVX registers, and is then transformed according to the used instruction in a single operation. This may take multiple clock cycles, but is usually faster than performing the operation sequentially for the inputs using normal instructions. Due to the hardware specialization and increased throughput, using AVX instructions may consume more power than normal instructions, causing a frequency throttling to maintain Thermal Design Power (TDP) limits. \cite{avx_whitepaper} \\
			
			The AVX-512 extensions enlarge the existing AVX2 extensions from 256-bit to 512-bit vector lengths and introduce many new characteristics and instructions. The new AVX-512 vpcompress instructions perform exactly a compressstore operation. Elements in the input vector have to all be of the same length but there are several instructions available to deal with different lengths of elements. A seperate AVX register holds the bitmask used in the operation. \\
			
			Although AVX2 does not have a dedicated compressstore instruction, the behaviour of a compressstore operation over the short vector register may be reproduced using multiple AVX2 instructions. \cite{avx2_leftpack, cern_datastreaming} \\
		
		\section{CUB}
			CUB is a library of reusable components for programming CUDA devices. It provides features for sorting, reducing, performing prefix scans, histogram creation and also a compressstore. \cite{cub} \\
			The algorithms used in the CUB components have been fine tuned for usage with many different CUDA GPUs, and are specialized for operation not only device wide, but also spanning just a warp or block. \\
			
			The kernel used by CUB to perform the compressstore operation is designed to only schedule one wave of blocks to the device. That is, there are enough blocks to fill the device, but no additional blocks waiting to be scheduled onto an SM. This ensures that all synchronization can be done at runtime on the device, instead of relying on multiple kernel launches to synchronize dependency chains. \\
			
			It is imperative to note that the CUB \texttt{flagged} functionality not only performs a compressstore, but also places all masked out elements in reverse order to the back of the output vector. \\
		
	\chapter{Analysis and Concepts}
		The general compressstore problem can be split into three smaller sequential partial tasks. The first two of which only perform actions on the bitmask and intermediate constructs, to enable simple parallelization of the actual writeout task, which touches the most data. \\
		
		The seemingly inherently sequential property of the compressstore, being the dependency of writeout location for an element on the count of enabled elements linearly before it, can be parallelized using a reduction. \\
		Specifically, every enabled element is written out at an offset corresponding to the exclusive prefix sum of the bitmask at its position. \\
		
		However, because of the large amount of elements that might be processed, it is not feasible to perform any algorithm on single elements. Instead, input elements and bitmask bits are chunked together into chunks of a variable length. This drastically helps reduce the problem size for all three steps. \\
		
		To calculate the prefix sum for chunks, a population count of enabled bits for all preceeding chunks is required. As such the first step is to perform a popcount kernel over all chunks. Since this only requires reading from the bitmask it does not touch a lot of data. \\
		
		A prefix sum scan operation may be parallelized using a reduction and an expansion. The expansion step is not always necessary, as the (exclusive) prefix sum for a chunk can also be calculated using just the intermediate data from the reduction and another algorithm to perform the collection of the correct values. \\
		
		When the prefix sum is completely or partially done, the third task is to perform the writeout of elements per chunk using the pre-calculated prefix sums as the chunks' starting offset. This leaves the writeout task easily parallelizable, which is important because it is also the task that touches the most data. \\
		
		The emphasis on reducing the amount of touched data is crucial, because for the compressstore operation there are much more memory accesses than actual calculations performed. If the writeout work can be simplified by taking more time to analyze the much smaller bitmask, considerable time may be saved, especially for very sparse masks of various distributions.
	
		When working on bitmasks of repeating patterns, as they are created by a project operation, none of the preparatory steps are neccessary. The writeout location of each input element is directly available via calculation. To reduce the amount of dormant threads in a naive monolithic or grid-striding implementation of this for low density bitmasks, a skip table can be precalculated for the pattern, such that the writeout memory access is always the width of all 32 threads of a warp writing at once their corresponding input element. \cite{cuda_gridstriding} \\
		This chunk wise algorithm can then be applied in chunks of elements over the entire input data, requiring no reading of a large bitmask, because the pattern is known beforehand. \\
	
		\section{Popcount}
			The popcount kernel uses a variable chunklength to perform a population count of enabled bits over chunks of the input bitmask. The actual popcount operation is performed on unsigned 32-bit integers using a CUDA intrinsic function. \\
			Each thread processes one chunk of the bitmask, while adjacent threads process adjacent chunks. The output of the popcount ensures large width memory accesses, because all threads store directly next to each other and at once. \\
			
			The writeout phase of the popcount kernel is also a good place to perform a rudimentary histogram creation if required. To distinguish between different distributions of the bitmask with the same $p$ near $0.5$, further explanation is given in section \ref{sec:analysis_writeout}. \\
		
		\section{Prefix Sum Scan}
			The exclusive prefix sum as required, denotes for each chunk entry the sum of all entries before it. The exclusive prefix sum does not contain the own entry in the prefix sum for a given entry, so it starts at 0. \\
			
			The prefix sum scan can be split into two phases when parallelized. The up-sweep reduction phase is imagined like a tree such as in figure \ref{fig:pss_up_sweep}. Every entry holds at the end, the inclusive prefix sum of the subtree that reduced to it. Because of the nature of the binary reduction, the inner nodes of the reduction tree correspond to repeating offsets at various powers of two. This is an intermediate product and only becomes useful through some form of down-sweep computation. \\
			The down-sweep phase distributes the values of the up-sweep, through the inner nodes, down into the leaves of the reduction tree, thereby creating the final exclusive prefix sum over all entries. \cite{cuda_gems3ppss} \\
		
			\subsection{Up-Sweep Reduction}
				One launch of the up-sweep reduction kernel, corresponds to one layer in the prefix sum reduction tree. The kernel is launched $\log_2(\text{total element count})$ times. \\
				To perform one layer of reduction, for every two consecutive tree nodes, their sum is stored in the location of the right node, which will correspond to a node in the next layer. Growing with the increasing depth of the reduction, towards the root, are the offsets between \emph{adjacent} nodes, i.e. those that need to be summed. Starting at depth zero for the leaf nodes, increasing by one for every layer, the $n$-th left-child element can be found at index $(2n+1) \cdot 2^d -1$ for depth $d$. The adjacent right element is found $2^d$ elements further to the right. See figure \ref{fig:pss_up_sweep} for a visualization of this up-sweep phase. \\
				\begin{figure}[!ht]
					\centering
					\includegraphics[width=0.95\linewidth]{./img/pss_up_sweep.png}
					\caption{\label{fig:pss_up_sweep}Visualization of the up-sweep operation performed by the prefix sum. The left side represents normal operation on a vector with a power of two element count. The right represents adjusted operation to accomodate for non power of two vector sizes. Marked in red is the out of bounds storage location, holding at the end the total amount of one bits. Grayed out cells represent the theoretical power of two cells, no memory is allocated to them.}
				\end{figure}
				
				The above algorithm only works on chunk counts equal to a power of two. It can easily be adapted to work with any count of chunks by introducing two more checks. \\
				If both the left and the right subtree indices are out of bounds, their result does not need to be recorded, since it is always zero, and is only going to be propagated further right as well. There would never be any new information introduced that can not be derived from the index, i.e. zero. \\
				There exists at most one pair of subtrees per depth layer, such that the left index exists and the right index is out of bounds. In this case, the value of the left subtree, and the right subtree (zero), can not be stored in the location of the right subtree, and is thus stored in a special out of bounds location, just for this case. Incidentally, after the algorithm is finished on a non-power of two chunk count array, or is run again with depth increased by one on a power of two chunk count array, the resulting total number of one bits is stored in the out of bounds location. \\
			
			\subsection{Down-Sweep Collection}
				\label{sec:analysis_pss_down_sweep}
				The down-sweep of the two phase prefix sum algorithm, functions essentially like a reverse reduction, distributing the partial prefix sum values from the inner nodes towards the leaf nodes. This is work efficient, as no operations are carried out multiple times without need. The expansion can be performed in place on the array of chunk entries and results in a concrete per chunk prefix sum value. \\
				Unfortunately this operates on the assumption that every chunk's prefix sum is actually necessary, and introduces again only limited use of the massive parallelism the GPU offers. Just like a reduction, many threads would be dormant for most of the time, or if done in layers, there would be many very sparse layers. \\
				
				To circumvent this issue, we devise an algorithm to compute, for a single chunk, its prefix sum value, using only the partial results of the up-sweep phase. \\
				This algorithm is very memory intensive, just like the expansion. Even performing it on all chunks individually might not incur much of the disadvantages, though it will introduce work inefficiency. Adjacent threads will access the same memory most of the time, and the computations are very light. \\
				
				\begin{figure}[!ht]
					\centering
					\includegraphics[width=0.45\linewidth]{./img/pss_pssidx.png}
					\caption{\label{fig:pss_pssidx}Visualization of the single chunk down-sweep calculation, performed for the element highlighted with a blue arrow. Below the partial prefix sum buffer is a virtual representation of the tree that leads to it. Green boxes in the tree mark accepted (fitting) sub trees, red boxes mark rejected ones. The vertical blue dotted line ends the range into which sub trees are fit. \\ Note that the rejected red subtree is the \emph{only} rejected subtree, no subtree without a box in the figure was considered, as described in the algorithm.}
				\end{figure}
			
				The algorithm essentially tries to fit as many, as large as possible, left sided subtrees into the index of the chunk to compute. Inputs are the chunk index, the partial prefix sum array and half of the smallest power of two that is greater or equal to the chunk count. The working operation is described in the algorithm \ref{alg:pssidx}. \\
				
				\begin{algorithm}[!ht]
					\SetAlgoLined
					\KwIn{cid, id of the target chunk}
					\KwIn{p2, half of the smallest power of two that is greater of equal to the chunk count}
					acc $\gets 0$; \\
					consumed $\gets 0$; \\
					\While{p2 $>= 1$}{
						\If{cid $>= consumed + p2$}{
							add value of prefix sum subtree at [consumed$+$p2$-1$] to acc; \\
							add p2 to consumed; \\
						}
						divide p2 by 2; \\
					}
					\KwOut{acc, prefix sum of the target chunk}
					\caption{\label{alg:pssidx}On-the-fly Prefix Sum Computation}
				\end{algorithm}
			
				This also works on non-power of two chunk counts, without adjustments. \\
				The power of two that is halved with every iteration (depth level), can be imagined as a subtree that has at its leaf level a width of p2. The colored boxes in figure \ref{fig:pss_pssidx} represent the leaf level width (leaf length) of the subtrees being fit. The boxes are placed at the highest node of their respective subtree, emphasizing the focus on fitting the largest possible subtrees first, which include smaller subtrees that then do not need testing. \\
			
		\section{Writeout}
			\label{sec:analysis_writeout}
			A naive approach to perform the writeout of enabled elements to the output, would be to let adjacent threads work on seperate (possibly adjacent) chunks of input data in parallel. This does not however accomodate the fact that larger width (coalesced) memory accesses perform generally much better on GPUs. The pipeline queue for the LSUs might quickly be filled and lead to warps actually stalling the SM. \\
			
			Alternative to the chunks per thread approach, there is a chunk per threads approach. Here one chunk of 1024 input elements is processed at once, by one full warp of threads. There is no inter warp-synchronization required, as all thread-to-thread communication is only performed intra-warp. \\
			The algorithm chosen starts by loading 32-bit long parts of the bitmask, one per thread of the warp, into shared memory. This makes up 1024 total bits loaded, and explains the fixation on 1024 input elements processed per chunk. \\
			To coalesce memory accesses into larger widths, for greater GPU memory performance, the threads of a warp collectively process steps of 32 input elements at once, as many as are enabled. Unfortunately this may lead to half a dormant warp per (up to) 32 element writeout in the general $p=0.5$ case. However all threads store data adjacent to each other and at once, and can as such be grouped up to fewer, larger width memory transactions which benefits the GPU performance. \\
			To resolve whether or not a thread has to participate in a given 32 element processing step every thread can use the bitmask stored in shared memory to check if the bit corresponding to its position is enabled. The position in the output a certain thread takes when writing out its element can be solved with a population count of one bits. Counting is done over all bits positionally before the bit corresponding to the thread. These elements, found linearly before a thread's own target element in the 32 element step, form again a warp local exclusive prefix sum. \\
			It is not necessary to calculate the prefix sum for all of the chunks processed by a warp. If the chunks are arranged in a continuous fashion, only the prefix sum of the first chunk is required. \\
			
			As mentioned above, the prefix sum for a chunk of elements to process may not always be actually required. For optimization of very sparse bitmasks, a secondary popcount in a seperate buffer may be useful, as processing of a chunk may be entirely skipped if the popcount entry in that chunk is zero, i.e. there are no elements to process. \\
			Because performing another popcount kernel pass is not free, it is useful that the total amount of one bits is already known after the prefix sum up-sweep. This information can be used to determine if a second popcount pass would gain any or enough performance benefits to justify its cost. \\
			Without using a histogram on chunks, for non-uniformly distributed bitmasks like a zipf or burst distribution, the amount of total one bits may not be enough to immediately tell that the optimization targeted popcount pass would be worth it. \\
			
			For repeating pattern bitmasks, writeout of the input elements can be optimized to always use all 32 parallel threads of a warp with adjacent writeout locations to accomodate larger width memory transactions. There are no dormant threads on a 32 element writeout threads, except possibly at the end of a chunk. \\
			To achieve this behaviour, the threads of a warp need to process 32 elements which will be adjacent to each other in the output at once per step. After calculating the position of the first one bit per thread, they use a 32 element lookup table that can be precomputed before the kernel launch on the CPU. The table denotes for every one bit, the amount of indices to skip such that 31 intermediate one bits have been skipped over. Using the intra-chunk writeout offset modulo the pattern length returns the entry to check in the table, the value of which is added to the offset. \\
		
		\section{Streaming}
			In the algorithm implemented through the above mentioned kernels, there exists a dependency of the writeout kernel on the prefix sum scan, which in turn requires values from the popcount kernel before being able to do its work. This dependency is, however, only really true for the current, and all preceeding chunks, i.e. a writeout of a chunk somewhere in the middle of all elements, does not depend on the prefix sum of chunks behind it, or their popcount. It is merely required that all chunks up to itself have a finished prefix sum. \\
			Because for some small total element counts, the popcount and prefix sum kernels might not be enough to saturate the entire device, this approach is to use streaming to start earlier with the more expensive writeout of chunks whose prefix sum is already finished. \\
			In this way, the theoretical maximum of time that can be saved is the runtime of almost the entire popcount and prefix sum kernel. For large element counts however, there is no time saved because the popcount and prefix sum kernel themselves would be enough to saturate the device. The same approach could also be used to hide data transfer from the CPU to the GPU, by already starting computation on transferred chunks. \\
			
			To avoid synchronization between the CPU and GPU, the existing dependency of kernels can be modeled using CUDA events, which enable efficient synchronization on the GPU itself. This way the writeout kernel for a chunk is immediately scheduled when the corresponding prefix sum kernel finishes. \\
		
		\section{Summary}
			In conclusion, the compressstore operation can be split into three dependent tasks, i.e. the popcount, the prefix sum scan and the writeout. Workload can be made easier for the intermediate steps by introducing the concept of chunks, which are blocks of consecutive input data that are always processed together. \\
			The workings of a two phase prefix sum scan are also explained, providing opportunity to skip the second pass and instead calculate it on the fly per chunk. \\
			Acceleration of sparse bitmasks has also been considered. For the general case, two approaches to the writeout task are described, aiming for performance improvements via coalescing memory accesses. \\
		
	\chapter{Evaluation}
	
		\section{Experimental Setup}
			All implementations have been tested on a RTX Quadro 8000 GPU, installed on a server provided by the chair of database theory, running a linux operating system. This GPU has 72 SMs and 48GB of GDDR6 global memory with a theoretical bandwidth of about 672 GB/s \cite{quadrortx8000_specs}. All tested algorithms used 64-bit integers as benchmark data to perform the compressstore on. \\
			
			To individually test the developed kernels, ignoring any PCIe data transfer times, CUDA events were used. Placing one event just before, and another just after the kernel launch, allows precise timing of runtime on the GPU. Where possible, grid- and block-dimensions as well as chunk lengths were varied. The tested kernels are:\\
			\begin{itemize}
				\item Popcount kernel \\ (\texttt{3pass\_popc\_none})
				\item Partial prefix sum kernel over all required depths, using global memory \\ (\texttt{3pass\_pss\_gmem})
				\item Per chunk prefix sum completion, using global memory \\ (\texttt{3pass\_pss2\_gmem})
				\item Naive chunks per thread writeout kernel, using \textbf{p}artial and \textbf{f}ull prefix sum buffers \\ (\texttt{3pass\_\textbf{p}proc\_none} and \texttt{3pass\_\textbf{f}proc\_none})
				\item Optimized chunk per threads writeout kernel, using \textbf{p}artial and \textbf{f}ull prefix sum buffers \\ (\texttt{3pass\_\textbf{p}proc\_true} and \texttt{3pass\_\textbf{f}proc\_true})
			\end{itemize}
			In all graphs if not otherwise indicated, the best performing grid-/block-dimensioning and chunk lengths are represented. \\
			
			To provide a baseline comparison for the developed kernels, both CPU and GPU based existing approaches are tested:
			\begin{itemize}
				\item CPU based singlethread \\ (\texttt{cpu})
				\item CPU AVX base singlethread \\ (\texttt{avx512})
				\item GPU based singlethread \\ (\texttt{single\_thread})
				\item GPU based CUB flagged operation, using a bytemask \\ (\texttt{cub\_flagged\_bytemask})
				\item GPU based CUB flagged operation, using an iterator over a bitmask \\ (\texttt{cub\_flagged\_biterator})
				\item GPU based CUB prefix sum scan \\ (\texttt{cub\_pss})
			\end{itemize}
			
			To provide a more complete comparison against the baselines and each other, some graphs use \emph{packages}, i.e. grouping runtimes of kernels as they would be used in a proper scenario to perform the compressstore operation. The packages represented are:
			\begin{itemize}
				\item Streaming using the best stream count and best in class grid/block/chunk-dimensioning \\ (\texttt{async\_streaming\_3pass})
				\item Stand-alone processing using the optimized writeout kernel and two phase prefix sum calculation \\ (\texttt{3pass\_fproc\_true\_sa})
					\subitem = 2 * \texttt{3pass\_popc\_none} + \texttt{3pass\_pss\_gmem} + \texttt{3pass\_pss2\_gmem} + \texttt{3pass\_fproc\_true}
				\item Processing using the optimized writeout kernel and a partial prefix sum \\ (\texttt{3pass\_pproc\_true\_sa})
					\subitem = \texttt{3pass\_popc\_none} + \texttt{3pass\_pss\_gmem} + \texttt{3pass\_popc\_none} + \texttt{3pass\_fproc\_true}
				\item Processing using the optimized writeout kernel and a full prefix sum provided by CUB \\ (\texttt{3pass\_fproc\_true\_cub})
					\subitem = \texttt{3pass\_popc\_none} + \texttt{cub\_pss} + \texttt{3pass\_popc\_none} + \texttt{3pass\_fproc\_true}
			\end{itemize}
	
		\begin{figure}[!ht]
			\centering
			\includegraphics[width=0.9\linewidth]{./img/runtime_over_datasize_normal.png}
			\caption{\label{fig:runtime_over_datasize_normal}Logarithmic runtime of all tested components over increasing datasize. Fixed $p=0.5$ at a chunklength of 1024 where applicable.}
		\end{figure}
	
		Both CPU based approaches, as well as the GPU based \texttt{single\_thread}, performed at all times slower than all other kernels that properly utilize the GPU's parallelism. \\
	
		\section{Popcount}
			\label{sec:evaluation_popcount}
			The popcount kernel takes up relatively little runtime in the total compressstore operation. Because there is no comparison for it and the performed actions are simple, the runtime can be considered almost negligible. \\
			The memory access pattern for the chunkwise writeout of the popcount result deliberately accomodates for the GPUs preference towards larger width load/store transactions. \\
			
			Additional measurements show that a minimal modification of the algorithm, to utilize a 64-bit integer popcount instruction, almost halves the runtime. To preserve compatibility with 32 bit long chunks, additional guards have to be inserted for the edge cases. Across 8GiB of input data, this is an improvement of roughly $0.25\text{ms}$ on the provided hardware, roughly 1.5\% of the total compressstore runtime. \\
			
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/runtime_heatmap_dimensioning_3pass_popc_none.png}
				\caption{\label{fig:runtime_heatmap_dimensioning_3pass_popc_none}Runtime heatmap of various block- / grid-dimensions for the popcount kernel. Fixed at 8GiB of data and $p=0.5$ with a chunksize of 1024 bits.}
			\end{figure}
			
			The heatmap in figure \ref{fig:runtime_heatmap_dimensioning_3pass_popc_none} shows the runtime of the popcount kernel in ms, varying over some powers of two for both grid- and block-dimensioning. A slight saturation wave pattern is visible, showing that there is an optimal range of threads per SM at which the algorithm performs best. The pattern is visible as a diagonal line because both axes are logarithmic with the base two. \\
			Using less block and/or threads per block degrades performance substantially, as the device can not be saturated, and latencies can not be hidden. Using too high dimensioning, the performance drops again, even if very subtly, due to increased scheduling effort with no benefit in saturation. \\
			The most interesting observation to be made is the relatively large drop in performance when using 1024 threads per block instead of a lower power of two. The Nsight Compute report showed the difference between these to be an almost 60\% worse L1 cache hit rate from 1024 threads per block version compared to the 512 dimensioning. The dramatically decreased hit rate also caused the LSU transaction queue to fill more frequently, stalling the SM for much longer, leading to yet again decreased performance. \\
			
			\subparagraph{Further experimentation} on this kernel may be targeted towards increasing the usage efficiency of the LSU. \\
			In its current form the popcount kernel loads for every thread, data from a different chunk. With older GPUs this results in the LSU loading larger widths of data than are actually required. In the best case, this extra loaded data could then be reused from cache when each thread gets to processing its next few elements. Unfortunately, GPU caches, especially the L1 caches inside the SMs, are very small, even more so when considering that they are shared among all the resident threads. Due to the low computational requirements of each kernel, lots of threads will be alive per SM. This will likely cause many L1 cache misses, leading to even more unneccessary loads. \\
			On more modern GPUs, the LSUs may perform smaller width loads of down to only 32 bytes each. This may alleviate some of the cache issues, however due to the many threads alive, the LSU's queue of transactions can quickly fill up and cause actual stalling of the SM, while threads wait for the queue to empty again. \\
			One way to reduce the impact of this issue would be to perform adjacent loads using all 32 threads of a warp, and process consecutive chunks by sliding the window of popcounting threads over them. The little synchronization required to collect the results from every thread can easily be hidden by the many memory transactions of other threads. \\
		
		\section{Prefix Sum Scan}
			Just like the popcount kernel, the two phase prefix sum scan, and especially the up-sweep phase take up very little of the total runtime of the compressstore operation. Additionally, both phases work only on the prefix sum buffer, which only has one entry per chunk. As chunklength increases, the amount of data touched to perform the prefix sum drop dramatically. Both phases work solely on global memory. \\
		
			As visible in the later shown figures \ref{fig:package_runtime_over_p} and \ref{fig:package_throughput_over_datasize_normal} there is barely any difference between \texttt{3pass} packages using the stand-alone prefix sum versus that calculated by cub. This also extends to the use of the partial prefix sum. \\
		
			\subsection{Up-Sweep Reduction}
				Due to the large offset and stride of the elements accessed by each thread, there is no pattern to coalesce memory transactions into, thus resulting in lots of unneccessary data loaded. This kernel is again very memory bound by the many transactions performed, as there is also very little computation happening. \\
				This issue is usually, and most commonly for reductions, weakened in impact by using shared memory. This would cluster kernel runs for a certain depth together into one run, and every block then needs only load the data once at the beginning and store once at the end. All intermediate depths compute on the part of the prefix sum buffer available in shared memory, at the single cycle per access cost. \\
				Because of the very low runtime part this has in an overall package, this optimization was not investigated further. \\
			 
			\subsection{Down-Sweep Collection}
				The down-sweep phase of the prefix sum utilizes the special method explained in section \ref{sec:analysis_pss_down_sweep} to compute a per chunk final prefix sum, using the partial data from the up-sweep phase. The writeout pattern of the prefix sum result per chunk is as large as possible, accomodating again, the GPUs preference towards larger width memory transactions. \\
				Using this approach the data loaded to compute the prefix sum for every chunk introduces unneccessarily loaded bytes. However, the result of a load can, because adjacent threads process adjacent chunks, almost always be used by an entire warp of threads simultaneously. In the best case, other threads of the same block may reuse the data from the L1 cache. This is because for many of the depth iterations in the algorithm, the sub trees fitted into the target chunk position are shared for all of the thread's chunks. \\
				
				This kernel touches even less data than the up-sweep phase, and has an even lower runtime. Even though both phases operate only on global memory, any gains by modifying them to use shared memory, are negligible due to their low runtime contribution to the total package.
		
		\section{Writeout}
			The writeout kernels take up by far the largest amount of runtime of any compressstore package. Considering the very small impact of both the popcount and prefix sum kernels, this section compares packages against each other.
		
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/package_runtime_over_p.png}
				\caption{\label{fig:package_runtime_over_p}Runtime of some compressstore packages over varying values of $p$, fixed at 8GiB and 1024 bit chunklength where applicable.}
			\end{figure}
			
			As mentioned in the analysis of the writeout kernel, the naive chunks per thread implementation has a much worse memory access pattern than the optimized chunk per threads kernel. The measured impact of this is a roughly 10-fold increase in runtime. No package using the naive approach is shown in figure \ref{fig:package_runtime_over_p}, as it would only degrade the ability to compare the optimized version with CUB. \\
			
			As figure \ref{fig:package_runtime_over_p} clearly shows, the 1024 zero bit optimization works very well to drastically reduce the runtime for lower $p$ values. In the more dense ranges of $0.5$ and upward, the CUB algorithm performs slightly more favorably, taking only a few milliseconds less runtime. \\
			
			Between the stand-alone prefix sum and using the optimized CUB functionality, there is barely a noticable difference in package runtime. This holds true for full and partial processing, the latter saving in theory a few more calculations and memory loads. \\
			
			Regarding $p$ values around $0.5$ and higher, performance for the \texttt{3pass\_proc\_true} packages is very near that of CUB. One possible reason for the developed algorithm to still perform worse may lie in the fact that there are at many times, especially near the $p=0.5$ value, many dormant threads in a warp per writeout pass. This is caused by the internal no synchronization principle of the algorithm, as explained in section \ref{sec:analysis_writeout}. \\
			One possible way to avoid this would be the precomputation of all writeout positions using the bitmask from shared memory, requiring another 1-2KiB of register/shared memory storage per warp of a block. The increased maximum width writeout size for every warp writeout cycle may clear space in the LSU transaction queue and reduce related stalls. Additionally the precomputation of the writeout positions can provide opportunity to hide latency of other memory operations, as otherwise this kernel is very memory bound and stalls dozens of cycles between issuing instructions. \\
			This optimization could give rise to another potential saving. If the whole bitmask of a chunk to precompute is loaded into shared memory, then it can also directly be used to detect much finer empty and full bitmask patterns. Instead of investing a whole popcount run just to detect empty chunks, the precomputation step includes this at essentially no extra cost. \\
			
			\begin{table}[!ht]
				\centering
				\begin{tabular}{c | c c c}
					\hline
					Algorithm & Uniform ($p=0.50$) & Burst ($p=0.50$) & Zipf ($p=0.29$) \\
					\hline
					3pass & 26.58 & \textbf{18.90} & \textbf{19.76} \\
					CUB & \textbf{23.50} & 23.78 & 21.59 \\
					\hline
				\end{tabular}
				\caption{\label{table:masktype_perf_comp}Runtime in milliseconds of \texttt{3pass\_fproc\_true\_cub} with its best-in-class configuration and \texttt{cub\_flagged\_biterator}, across three different masktypes. The given $p$ value of each masktype tells the total portion of one bits across the 8GiB dataset for the particular distribution.}
			\end{table}
			
			Optimizing runtime for 1024 bit long zero streaks works well on uniformly distributed bitmasks with very low $p$ values. However the $p$ value alone can only tell when this optimization will definitely improve performance, not rule out potential in itself. In the table \ref{table:masktype_perf_comp} the runtime values for zipf and burst type bitmasks are given for the best performing CUB baseline, and the best performing \texttt{3pass} implementation. \\
			Both the zipf and burst bitmask have a very high $p$ value, but they are not uniformly distributed, so they still score very high numbers of skippable 1024 zero bit streaks. \\
			Performance for the burst mask is slightly higher than the zipf mask. Although this is implementation specific, the tested GPU schedules blocks with lower indices earlier, causing almost all non-skippable chunks to be processed first for the zipf mask and leaving almost only empty chunks to be processed, which take a lot more computational resources than memory loads, due to the optimization. The burst mask does not always suffer this problem, due to the very long passages of only ones or only zeroes being distributed somewhat evenly across all chunks. This ensures there is always memory intensive writeout work, and computation focused skipping happening. \\
			In both special cases, the \texttt{3pass} kernels run faster than CUB, in the general case of uniform $p=0.5$ distribution CUB is slightly faster than \texttt{3pass}.
		
		\section{Streaming}
			Comparing the streaming package, to the other CUB and \texttt{3pass} packages, reveals more differences. \\
			
			\begin{figure}[!ht]
				\centering
				\includegraphics[width=0.9\linewidth]{./img/package_throughput_over_datasize_normal.png}
				\caption{\label{fig:package_throughput_over_datasize_normal}Throughput of some compressstore packages over increasing datasize, fixed at $p=0.5$ and 1024 bit chunklength where applicable.}
			\end{figure}
			
			The streaming compressstore package performs much slower than any of the packages using the optimized writeout kernel. Because it profits itself from this kernel, on very low $p$ values, streaming still runs quicker than CUB. \\
			The figure \ref{fig:package_throughput_over_datasize_normal} shows that the streaming approach only starts maximizing its potential at around 1GiB of total data size. Various powers of two have been tried for the stream counts, every data point represents the best performing stream count at that position. \\
			
			The large performance loss for small datasizes is likely owing to the extra synchronization required to launch the kernels seperately on many streams. The CUDA events are placed in a way that they can be recorded and waited for on the GPU using no additonal host side processing. The small datasize of every streams portion of the total data causes large portions of the devices processing and memory power to remain unused. Most of the runtime is supposed to be used up during writeout, which has to synchronize with previous prefix sums. Per small portion alone there is just not enough work available to saturate the devices capabilities. \\
			
			From the figure \ref{fig:package_throughput_over_datasize_normal} it can also be seen that the \texttt{3pass} kernels perform slightly better than the CUB packages at very low datasizes. \\
		
		\section{Summary}
			Overall the performance of the new hand crafted kernels matches that of CUB, the best baseline approach, very nearly in many cases. In tests involving very sparse bitmasks, or masks with ranges of low density, the performance of the developed packages is several times greater than that of CUB. \\
			All CPU and GPU based single thread approaches perform much worse than both CUB or \texttt{3pass}. \\
			
			Some problematic areas of the \texttt{3pass} kernels have been identified, and possible starting points for future research are provided. \\
		
	\chapter{Conclusion}
		We have analyzed the compressstore problem and introduced a new set of kernels to exploit the GPUs parallelism to accelerate the total runtime of the operation. Along the way we explained some of the design decisions targeted towards increasing performance on the GPU. \\
		The \texttt{3pass} kernels developed in this work perform best on inputs with bitmasks that contain areas of extreme scarcity. This feature can be exploited on a local level, as well as on a global level where easy detection is possible. As shown, sparse bitmasks enable the created kernels to perform the compressstore operation in a faster runtime than any of the baselines. In the general uniformly distributed $p=0.5$ case the improvement over singlethreaded CPU and GPU approaches persists, but runtimes of \texttt{3pass} prove slightly slower than that of CUB for these denser masks. \\
		
		\paragraph{Related Work} Apart from the existence of CUB, that provides compressstore functionality using the GPU, there seems to be relatively little related work available regarding this topic. Particularly pertaining to GPU acceleration of this issue, we have found none accessible at the time of writing. \\
		As mentioned in the introduction, the use of the AVX-512 compressstore instruction for reducing problem sizes for subsequent steps in a high throughput detector is employed at CERN \cite{cern_datastreaming}. This problem size reducing property is also particularly important for any GPU based database processing. As PCIe transfer is quite slow, data on the GPU should be used in the most efficient way possible. Performing early projection operations by using a compressstore instruction can save valuable space for parallel processing of transactions. \\
		The Template Vector Library described in the work of \citeauthor{tvl} \cite{tvl} describes an approach to database operations that is hardware oblivious. This abstraction layer is complemented by hardware conscious plug-ins, which can then be optimized much further for their respective hardware than a general approach permits. In a theoretical CUDA plug-in to the Template Vector Library, either CUB or \texttt{3pass} might be up for selection when issuing the operation, depending on the situation. This decision can be made by using information potentially available through previous processing steps, before the compressstore operation is performed. \\

		\paragraph{Future Work} The evaluation section already mentioned some possible approaches to further increase throughput of the kernels used in \texttt{3pass}. Especially for those regarding the first two preparation passes, popcount and prefix sum creation, the fraction of the total runtime is so small that most improvements would yield little impact. Nevertheless for operations where many smaller compressstore utilizing transactions are performed in parallel on the same GPU, these optimization might quickly add up. \\
		As explained in section \ref{sec:evaluation_popcount} when adopting a more rigid chunk length sizing, 64-bit popcount instructions prove almost twice as fast as 32-bits. In the usual vein of optimizing memory accesses towards broader transactions, the per warp loads performed on the bitmask might benefit from being adjacent, relying less on the cache and giving more space to the LSUs. We do not think the extra synchronization required to perform the popcount of adjacently loaded bitmask data will be noticable. \\
		To improve memory efficiency for the prefix sum calculation as well, shared memory may be used to accelerate the reduction into less kernel launches. During this shared memory reduction a histogram may be created for some rudimentary types of mask chunks. The histogram would give a better understanding of the density distribution of selected elements in the mask. This could be used to make a more informed decision on whether to use CUB or \texttt{3pass} for the operation, before the bulk of the writeout work actually has to be invested. \\
		Aside from the mentioned static pattern optimization through a table of skips in the mask, the general case could benefit from a similar approach of keeping the whole 32 threads of a warp alive at writeout time. Precomputation of this extra information required is possible directly in the writeout phase, using data already available in shared memory. If the same stage could replace the second popcount needed for long 0 bit streak optimization is another issue in need of further investigation. \\
		
	\chapter{Bibliography}
		As of the submission date, all online resource weblinks are available and correct.
		\printbibliography[heading=none]
	
\end{document}
